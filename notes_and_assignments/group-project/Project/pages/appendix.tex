% 
% 
\appendix
% 
\section{A breif proof for Fourier Transform}\label{sec:Fourier}
% 
\paragraph{The continuous Fourier Transform of a function \( f(t) \) is given by:}
\[ F(\omega) = \int_{-\infty}^{\infty} f(t) \cdot e^{-i \omega t} \,dt \]
\paragraph{To prove this, we start with the Fourier series representation of a periodic function. Let \( T \) be the period of \( f(t) \). The Fourier series expansion is given by:}
% 
\[ f(t) = \sum_{n=-\infty}^{\infty} c_n e^{i \omega_n t} \]
% 
\paragraph{Here, \( c_n \) are the Fourier coefficients, and \( \omega_n = \frac{2\pi n}{T} \) are the angular frequencies.}
% 
\paragraph{Now, consider the limit as \( T \) approaches infinity, turning the sum into an integral:}
% 
\[ f(t) = \lim_{T \to \infty} \sum_{n=-\infty}^{\infty} c_n e^{i \omega_n t} \]
% 
\paragraph{This leads to the continuous Fourier Transform integral:}
% 
\[ F(\omega) = \lim_{T \to \infty} \int_{-\frac{T}{2}}^{\frac{T}{2}} f(t) e^{-i \omega t} \,dt \]
% 
\paragraph{As \( T \) approaches infinity, the limits of integration become \( -\infty \) to \( \infty \):}
% 
\[ F(\omega) = \lim_{T \to \infty} \int_{-\infty}^{\infty} f(t) e^{-i \omega t} \,dt \]
% 
\paragraph{This is essentially the definition of the continuous Fourier Transform. The limit is introduced to handle functions that are not strictly periodic.}
% 
\paragraph{The rigorous proof involves showing that this limit exists for a wide class of functions and exploring the convergence properties. It requires knowledge of mathematical analysis and complex analysis.}
% 
% 
% 
% 
% 
% 
\section{Data Set}
\paragraph{Due to the large number of datasets and the number of columns, we uploaded the data to GitHub, and below are links to the various datasets.}
\subsection{Data Original Source}
\paragraph{The data set we got from FootyStats.org and we saved them with csv format.}
\paragraph{\href{https://github.com/QianZeHao123/IMDS/tree/main/notes_and_assignments/group-project/src/Data}{Click the Hyperlink here and Review the orginal data set.}}
% 
% 
% 
\subsection{The processed dataset}
\paragraph{\href{https://github.com/QianZeHao123/IMDS/blob/main/notes_and_assignments/group-project/src/Results/2023_24_General.csv}{A data set for combining the 2023-24 season data and historical features.}}
% 
% 
\paragraph{\href{https://github.com/QianZeHao123/IMDS/blob/main/notes_and_assignments/group-project/src/Results/2023_24_Processed.csv}{Data set for caculating the Correlation between variables and Points}}
% 
% 
\paragraph{\href{https://github.com/QianZeHao123/IMDS/blob/main/notes_and_assignments/group-project/src/Results/2023_24_Processed_PCA.csv}{Data set for caculating PCA}}
% 
% 
\subsection{Dataset for Saving Results}
% 
% 
\paragraph{\href{https://github.com/QianZeHao123/IMDS/blob/main/notes_and_assignments/group-project/src/Results/Cor_result.csv}{Results about Correlation between variables and Points}}
% 
% 
\paragraph{\href{https://github.com/QianZeHao123/IMDS/blob/main/notes_and_assignments/group-project/src/Results/pca_result.csv}{Results about PCA}}
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
\section{Premier League Data Fetch Scripts}\label{sec:GetData}
\begin{lstlisting}[style=pystyle]
from selenium import webdriver
from selenium.webdriver.common.by import By
import re
from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support.select import Select
from selenium.webdriver.support import expected_conditions as EC
import time
import csv

options = webdriver.EdgeOptions()
options.add_experimental_option("detach", True)
driver = webdriver.Edge()
driver.maximize_window()
driver.get('https://footystats.org/england/premier-league')

start_year = 2007
end_year = 2023
Years = [
    f'{year}/{str(year+1)[-2:]}' for year in range(start_year, end_year + 1)]
# year = '2022/23'


for year in Years:
    select = driver.find_element(By.CLASS_NAME, "drop-down-parent.fl.boldFont")
    select.click()
    time.sleep(2)
    # Replace 'your_data_hash_value' with the specific value you want to select
    # chooseSeason = driver.find_element(By.)
    # get element
    element = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.LINK_TEXT, year))
    )
    element.click()
    time.sleep(2)
    # mp, win, draw, loss, gf, ga, gd
    # Find an element by its class (replace 'element_class' with the actual class of the element on the webpage)
    TEAMs = driver.find_elements(
        By.CLASS_NAME, 'bold.hover-modal-parent.hover-modal-ajax-team')
    MPs = driver.find_elements(By.CLASS_NAME, 'mp')
    WINs = driver.find_elements(By.CLASS_NAME, 'win')
    DRAWs = driver.find_elements(By.CLASS_NAME, 'draw')
    LOSSs = driver.find_elements(By.CLASS_NAME, 'loss')
    GFs = driver.find_elements(By.CLASS_NAME, 'gf')
    GAs = driver.find_elements(By.CLASS_NAME, 'ga')
    GDs = driver.find_elements(By.CLASS_NAME, 'gd')
    POINTs = driver.find_elements(By.CLASS_NAME, 'points.bold')
    # Get text content from the element
    team_list = []
    mp_list = []
    win_list = []
    draw_list = []
    loss_list = []
    gf_list = []
    ga_list = []
    gd_list = []
    point_list = []

    for team in TEAMs:
        text_content = team.text
        team_list.append(text_content)

    for mp in MPs:
        text_content = mp.text
        # Use regex to extract only numbers
        numbers = re.findall(r'\d+', text_content)
        # Concatenate the numbers if there are multiple matches
        valid_numbers = [int(num) for num in numbers if num]
        # Append the valid numbers to the list
        mp_list.extend(valid_numbers)

    for win in WINs:
        text_content = win.text
        # Use regex to extract only numbers
        numbers = re.findall(r'\d+', text_content)
        # Concatenate the numbers if there are multiple matches
        valid_numbers = [int(num) for num in numbers if num]
        # Append the valid numbers to the list
        win_list.extend(valid_numbers)

    for draw in DRAWs:
        text_content = draw.text
        # Use regex to extract only numbers
        numbers = re.findall(r'\d+', text_content)
        # Concatenate the numbers if there are multiple matches
        valid_numbers = [int(num) for num in numbers if num]
        # Append the valid numbers to the list
        draw_list.extend(valid_numbers)

    for loss in LOSSs:
        text_content = loss.text
        # Use regex to extract only numbers
        numbers = re.findall(r'\d+', text_content)
        # Concatenate the numbers if there are multiple matches
        valid_numbers = [int(num) for num in numbers if num]
        # Append the valid numbers to the list
        loss_list.extend(valid_numbers)

    for gf in GFs:
        text_content = gf.text
        # Use regex to extract only numbers
        numbers = re.findall(r'\d+', text_content)
        # Concatenate the numbers if there are multiple matches
        valid_numbers = [int(num) for num in numbers if num]
        # Append the valid numbers to the list
        gf_list.extend(valid_numbers)

    for ga in GAs:
        text_content = ga.text
        # Use regex to extract only numbers
        numbers = re.findall(r'\d+', text_content)
        # Concatenate the numbers if there are multiple matches
        valid_numbers = [int(num) for num in numbers if num]
        # Append the valid numbers to the list
        ga_list.extend(valid_numbers)

    for gd in GDs:
        text_content = gd.text
        # Use regex to extract only numbers
        numbers = re.findall(r'-?\d+', text_content)
        # Concatenate the numbers if there are multiple matches
        valid_numbers = [int(num) for num in numbers if num]
        # Append the valid numbers to the list
        gd_list.extend(valid_numbers)

    for point in POINTs:
        text_content = point.text
        # Use regex to extract only numbers
        numbers = re.findall(r'\d+', text_content)
        # Concatenate the numbers if there are multiple matches
        valid_numbers = [int(num) for num in numbers if num]
        # Append the valid numbers to the list
        point_list.extend(valid_numbers)

    # Print the list of extracted numbers
    print('TEAM List is:', team_list)
    print('MP List is:', mp_list)
    print('WIN List is:', win_list)
    print('DRAW List is:', draw_list)
    print('Loss List is:', loss_list)
    print('GF List is:', gf_list)
    print('GA List is:', ga_list)
    print('GD List is:', gd_list)
    print('POINT List is:', point_list)

    # CSV file
    year_file = year.replace('/', '_')
    csv_file_path = './' + 'Data/' + year_file + '.csv'

    # write data to csv
    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)

        # edit header
        header = ['Team', 'MP', 'Win', 'Draw',
                    'Loss', 'GF', 'GA', 'GD', 'Points']
        writer.writerow(header)

        # write data
        for i in range(len(team_list)):
            row = [team_list[i], mp_list[i], win_list[i], draw_list[i],
                    loss_list[i], gf_list[i], ga_list[i], gd_list[i], point_list[i]]
            writer.writerow(row)

    print(f'Data has been written to {csv_file_path}')

driver.quit()

\end{lstlisting}
% 
% 
% 
% 
\section{Feature Extract Function}\label{sec:FEF}
% 
% 
\subsection{extractFeatures.py}
\begin{lstlisting}[style=pystyle]
import pandas as pd
import numpy as np
from scipy.stats import skew, kurtosis
from scipy.fftpack import fft
from scipy.signal import find_peaks
from scipy.stats import entropy


def extract_features(signal):
    features = {}

    # Convert the column to numeric, handling non-numeric values
    signal_numeric = pd.to_numeric(signal, errors='coerce')

    # Replace or remove specific non-numeric values
    signal_numeric.replace('ALIGNED', np.nan, inplace=True)

    # Remove NaN values or use interpolation as needed
    signal_numeric.dropna(inplace=True)

    # Convert to NumPy array
    signal_array = signal_numeric.to_numpy()

    # Check if the array is not empty
    if len(signal_array) > 0:
        # Time-domain features
        features['mean'] = np.mean(signal_array)
        features['std_dev'] = np.std(signal_array)
        features['rms'] = np.sqrt(np.mean(np.square(signal_array)))
        features['skewness'] = skew(signal_array)
        features['kurtosis'] = kurtosis(signal_array)
        features['max_value'] = np.max(signal_array)
        features['min_value'] = np.min(signal_array)
        features['median'] = np.median(signal_array)
        features['iqr'] = np.percentile(
            signal_array, 75) - np.percentile(signal_array, 25)
        features['zero_crossing_rate'] = np.sum(
            np.diff(np.sign(signal_array)) != 0) / len(signal_array)

        # Frequency-domain features using FFT
        try:
            fft_result = fft(signal_array)
            magnitude_spectrum = np.abs(fft_result)

            features['dominant_frequency'] = np.argmax(magnitude_spectrum)
            features['max_frequency_magnitude'] = np.max(magnitude_spectrum)

            # Statistical measures
            # features['auto_correlation'] = np.correlate(
            #     signal_array, signal_array, mode='full')

            # Frequency-domain features
            features['power_spectral_density'] = np.mean(
                np.square(magnitude_spectrum))
            features['total_power'] = np.sum(np.square(signal_array))
            features['spectral_entropy'] = entropy(magnitude_spectrum)
            features['centroid_frequency'] = np.sum(np.arange(
                len(magnitude_spectrum)) * magnitude_spectrum) / np.sum(magnitude_spectrum)

            # Time-frequency features
            # Add your wavelet transform code here
            # Example using FFT for STFT
            # features['wavelet_coefficients'] = []
            # features['stft'] = np.abs(np.fft.fftshift(
            # np.fft.fft(signal_array)))

            # Other features
            # features['peaks_count'], _ = find_peaks(signal_array)

        except ValueError as e:
            print(f"Error in FFT calculation: {e}")

    return features
\end{lstlisting}
% 
% 
% 
% 
% 
% 
\subsection{Using Data Extraction Function}
% 
\begin{lstlisting}[style=pystyle]

import os
import pandas as pd
from extractFeatures import extract_features

# Folder path containing all CSV files
folder_path = "./Data"

# Target team
target_team = "Everton FC"
target_teams = ['Manchester City FC', 'Liverpool FC', 'Arsenal FC',
                'Tottenham Hotspur FC', 'Aston Villa FC',
                'Manchester United FC', 'Newcastle United FC',
                'Brighton & Hove Albion FC', 'West Ham United FC',
                'Chelsea FC', 'Brentford FC', 'Crystal Palace FC',
                'Wolverhampton Wanderers FC', 'Nottingham Forest FC',
                'Fulham FC', 'AFC Bournemouth', 'Luton Town FC',
                'Sheffield United FC', 'Everton FC', 'Burnley FC']
# List of all CSV file names
csv_files = [
    '2007_08.csv', '2008_09.csv', '2009_10.csv', '2010_11.csv',
    '2011_12.csv', '2012_13.csv', '2013_14.csv', '2014_15.csv',
    '2015_16.csv', '2016_17.csv', '2017_18.csv', '2018_19.csv',
    '2019_20.csv', '2020_21.csv', '2021_22.csv', '2022_23.csv'
]



def get_result_df(target_team):
    # Create an empty DataFrame to store the extracted data
    result_df = pd.DataFrame(
        columns=["File", "Team", "MP", "Win",
                    "Draw", "Loss", "GF", "GA",
                    "GD", "Points"]
    )
    # Loop through each CSV file
    for csv_file in csv_files:
        # Build the full path of the CSV file
        file_path = os.path.join(folder_path, csv_file)

        # Read the CSV file
        df = pd.read_csv(file_path)

        # Extract data for the target team
        team_data = df[df["Team"] == target_team]

        # If data for the target team is found, add it to the result DataFrame
        if not team_data.empty:
            # Use pd.concat to add the target team's data from the current file to the result DataFrame,
            # and add the file name column
            result_df = pd.concat(
                [result_df, team_data.assign(File=csv_file)], ignore_index=True)

    return result_df

# print(result_df)
# result_df.to_csv("output.csv", index=False)

selected_columns = ['Win','Draw','Loss','GF','GA','GD','Points']

dfs = []

for target_team in target_teams:
    result_df = get_result_df(target_team)


    features_dict = {}
    for column in selected_columns:
        selected_data = result_df[column]
        features_column = extract_features(selected_data)
        features_dict[column] = features_column

    # Flatten the nested structure and convert to a DataFrame
    flattened_features = {}
    for column, feature_dict in features_dict.items():
        for key, value in feature_dict.items():
            flattened_features[f'{column}_{key}'] = value

    flattened_df = pd.DataFrame([flattened_features])

    # Append the DataFrame to the list
    dfs.append(flattened_df)

# Concatenate all DataFrames into a single DataFrame

result_df = pd.concat(dfs, ignore_index=True)

df_23_24 = pd.read_csv('./Data/2023_24.csv')
result = pd.concat([df_23_24, result_df], axis=1)
# Export the result to a CSV file
result.to_csv("2023_24_General.csv", index=False)
\end{lstlisting}
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
\section{Correlation Calculating}\label{sec:CC}
% 
% 
% 
\begin{lstlisting}[style=pystyle]
# Import the pandas library
import pandas as pd

# Read the CSV file
# Replace '2023_24_Processed.csv' with the actual file name and path
df = pd.read_csv('./Results/2023_24_Processed.csv')

# Define the target column for Spearman correlation
target_column = 'Points'

# Calculate Spearman correlation between the target column and all other columns
correlation_result = df.corrwith(df[target_column], method='spearman')

# Create a DataFrame containing the correlation results
result_df = pd.DataFrame({
    'Positive Correlation': correlation_result[correlation_result > 0].sort_values(ascending=False),
    'Negative Correlation': correlation_result[correlation_result < 0].sort_values(ascending=True)
})

# Print the results
print(result_df)

# Save the results to a CSV file
result_df.to_csv('./Results/Cor_result.csv', index=True)

# Extract the variables with absolute correlation greater than 0.6
positive_correlation_list = correlation_result[(correlation_result > 0) & (correlation_result.abs() > 0.65)].sort_values(ascending=False)
negative_correlation_list = correlation_result[(correlation_result < 0) & (correlation_result.abs() > 0.65)].sort_values(ascending=True)

# Print the lists of variables with absolute correlation greater than 0.6
print(positive_correlation_list)
print(negative_correlation_list)

positive_correlation_list.to_csv('./Results/Positive_Cor_result.csv', index=True)
negative_correlation_list.to_csv('./Results/Negative_Cor_result.csv', index=True)


positive_correlation_variables = list(positive_correlation_list.index)
negative_correlation_variables = list(negative_correlation_list.index)


print(positive_correlation_variables)
print(negative_correlation_variables)
\end{lstlisting}
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
\section{PCA Analysis Implementation}\label{sec:PCA}
% 
\begin{lstlisting}[style=pystyle]
# Import the pandas library
import pandas as pd
from sklearn.decomposition import PCA


# Read the CSV file
# Replace '2023_24_Processed.csv' with the actual file name and path
df = pd.read_csv('./Results/2023_24_Processed.csv')


select_col = ['Win', 'GD', 'GF', 'GF_max_value', 'GD_max_value', 'Points_rms',
                'GF_mean', 'Points_mean', 'GF_rms', 'Win_rms', 'GF_median', 'Points_max_value',
                'GD_mean', 'Win_max_value', 'GF_power_spectral_density', 'GF_total_power',
                'Win_mean', 'GF_max_frequency_magnitude', 'Points_power_spectral_density',
                'Points_max_frequency_magnitude', 'Points_total_power', 'Win_total_power',
                'Win_power_spectral_density', 'Points_median', 'GF_min_value', 'GF_std_dev',
                'Draw_centroid_frequency', 'Loss', 'GA', 'Loss_min_value', 'Loss_mean',
                'Loss_rms', 'Loss_median', 'Loss_max_value']


selected_columns = df[select_col]
selected_columns.to_csv('./Results/2023_24_Processed_PCA.csv', index=False)

pca = PCA()
pca_result = pca.fit_transform(selected_columns)

pca_df = pd.DataFrame(data=pca_result, columns=[
                        f'PC{i+1}' for i in range(pca_result.shape[1])])
# result_df = pd.concat([selected_columns.reset_index(drop=True), pca_df], axis=1)

# result_df.to_csv('./pca_result.csv', index=False)
pca_df.to_csv('./Results/pca_result.csv', index=False)

\end{lstlisting}